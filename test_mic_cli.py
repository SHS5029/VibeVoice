#!/usr/bin/env python
"""VibeVoice ASR Microphone Test CLI"""
import sys, os, argparse, time as time_mod, numpy as np, queue

try:
    import sounddevice as sd
except ImportError:
    print("ERROR: pip install sounddevice"); sys.exit(1)
try:
    import soundfile as sf
except ImportError:
    print("ERROR: pip install soundfile"); sys.exit(1)
try:
    import torch
    HAS_CUDA = torch.cuda.is_available()
    GPU_NAME = torch.cuda.get_device_name(0) if HAS_CUDA else None
except ImportError:
    print("ERROR: pip install torch"); sys.exit(1)

try:
    from demo.vibevoice_asr_gradio_demo import VibeVoiceASRInference
except ImportError as e:
    print(f"ERROR: {e}\ncd VibeVoice í´ë”ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”"); sys.exit(1)

try:
    from vibevoice.llm_client import LLMClient
except ImportError as e:
    print(f"WARNING: LLMClient not available: {e}")

def list_devices():
    print("\n=== ì˜¤ë””ì˜¤ ì¥ì¹˜ ===")
    for i, d in enumerate(sd.query_devices()):
        if d['max_input_channels'] > 0:
            default = " (ê¸°ë³¸)" if i == sd.default.device[0] else ""
            print(f"  [{i}] {d['name']}{default}")

def record(duration=5, sr=16000, device=None):
    q = queue.Queue()
    def cb(data, frames, t, status):
        if status: print(f"ê²½ê³ : {status}", file=sys.stderr)
        q.put(data.copy())
    
    print(f"\në…¹ìŒ {duration}ì´ˆ - ë§ì”€í•˜ì„¸ìš”!")
    try:
        with sd.InputStream(samplerate=sr, channels=1, callback=cb, device=device, dtype='float32'):
            start = time_mod.time()
            while time_mod.time() - start < duration:
                r = duration - (time_mod.time() - start)
                p = int(20 * (1 - r/duration))
                print(f"\r[{'='*p}{'-'*(20-p)}] {int(r)+1}ì´ˆ  ", end="", flush=True)
                time_mod.sleep(0.1)
        print(f"\r[{'='*20}] ì™„ë£Œ!          ")
    except sd.PortAudioError as e:
        print(f"\në§ˆì´í¬ ì˜¤ë¥˜: {e}\n--list-devicesë¡œ í™•ì¸í•˜ì„¸ìš”"); sys.exit(1)
    
    chunks = []
    while not q.empty(): chunks.append(q.get())
    return np.concatenate(chunks) if chunks else None

def print_segments(segments):
    """Print transcription segments to console."""
    if not segments:
        print("\nğŸ“‹ Audio Segments: None available")
        return
    print(f"\nğŸ“‹ Audio Segments ({len(segments)} segments):")
    print("=" * 60)
    for seg in segments[:50]:  # Show first 50
        start = seg.get('start_time', 'N/A')
        end = seg.get('end_time', 'N/A')
        speaker = seg.get('speaker_id', 'N/A')
        text = seg.get('text', '')
        print(f"[{start} - {end}] Speaker {speaker}: {text}")
    if len(segments) > 50:
        print(f"  ... and {len(segments) - 50} more segments")
    print("=" * 60)

def print_analysis(analysis):
    """Print LLM context analysis to console."""
    print("\nğŸ” Context Analysis (AI):")
    print("=" * 60)
    print(f"ğŸ“‹ ìš”ì•½: {analysis['summary']}")
    print(f"ğŸ“ ìƒí™©: {analysis['situation']}")
    print(f"ğŸ“‚ ì˜ë„: {analysis['intent']}")
    print(f"ğŸ­ ê°ì •: {analysis['sentiment']}")
    print(f"âœ… ê¶Œì¥ ì¡°ì¹˜:")
    for action in analysis['next_actions']:
        print(f"   - {action}")
    print("=" * 60)

def main():
    p = argparse.ArgumentParser(description="VibeVoice ASR ë§ˆì´í¬ í…ŒìŠ¤íŠ¸")
    p.add_argument("--model_path", default="microsoft/VibeVoice-ASR")
    p.add_argument("--duration", type=int, default=5, help="ë…¹ìŒ ì‹œê°„(ì´ˆ)")
    p.add_argument("--device", default="auto", help="cuda/cpu/auto")
    p.add_argument("--mic", type=int, help="ë§ˆì´í¬ ì¸ë±ìŠ¤")
    p.add_argument("--list-devices", action="store_true")
    p.add_argument("--keep-audio", action="store_true")
    p.add_argument("--no-analysis", action="store_true", help="Skip LLM context analysis")
    args = p.parse_args()
    
    if args.list_devices:
        list_devices(); sys.exit(0)
    
    device = "cuda" if args.device == "auto" and HAS_CUDA else ("cpu" if args.device == "auto" else args.device)
    if device == "cuda" and not HAS_CUDA:
        print("CUDA ë¶ˆê°€, CPU ì‚¬ìš©"); device = "cpu"
    print(f"ì¥ì¹˜: {device}" + (f" ({GPU_NAME})" if device == "cuda" else ""))
    
    print(f"\nëª¨ë¸ ë¡œë”©: {args.model_path}")
    try:
        dtype = torch.bfloat16 if device == "cuda" else torch.float32
        asr = VibeVoiceASRInference(model_path=args.model_path, device=device, dtype=dtype, attn_implementation="eager")
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}"); sys.exit(1)
    
    audio = record(args.duration, device=args.mic)
    if audio is None:
        print("ë…¹ìŒ ì‹¤íŒ¨"); sys.exit(1)
    
    tmp = f"temp_mic_{int(time_mod.time())}.wav"
    sf.write(tmp, audio, 16000)
    
    print("\nì¸ì‹ ì¤‘...")
    start = time_mod.time()
    try:
        result = asr.transcribe(audio_path=tmp, max_new_tokens=512, do_sample=False)
    except Exception as e:
        print(f"ì „ì‚¬ ì‹¤íŒ¨: {e}")
        if os.path.exists(tmp): os.remove(tmp)
        sys.exit(1)
    
    print(f"\n{'='*50}\nê²°ê³¼:\n{'='*50}")
    print(result.get("raw_text", "(ì¸ì‹ ì—†ìŒ)"))
    print(f"{'='*50}\nì²˜ë¦¬: {time_mod.time()-start:.2f}ì´ˆ")

    # Display Audio Segments
    segments = result.get("segments", [])
    print_segments(segments)

    # Display Context Analysis (if not skipped)
    if not args.no_analysis and segments:
        try:
            llm_client = LLMClient()
            analysis = llm_client.analyze_call(segments)
            print_analysis(analysis)
        except Exception as e:
            print(f"\nâš ï¸  Context analysis skipped (error: {e})")
    elif args.no_analysis:
        print("\nâ­ï¸  Context Analysis skipped (--no-analysis flag)")

    if not args.keep_audio and os.path.exists(tmp): os.remove(tmp)

if __name__ == "__main__":
    main()
